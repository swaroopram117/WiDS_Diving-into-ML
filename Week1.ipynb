{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 1 : Linear Algebra and Calculus**\n",
        "\n",
        "In this assignment, we shall explore the concepts of analytic and numeric computation of gradients. Further, we will have a look at the computation graph which is a central concept to building a neural network. For learning how gradients are computed analytically, refer to the resources provided in this week."
      ],
      "metadata": {
        "id": "lORIgnt_yoZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQWsYeD8ZlYxFBB33qIn7bwQvP-KuvLZXJOoA&usqp=CAU\"\n",
        " style=\"float:center;width:50px;height:50px;\">"
      ],
      "metadata": {
        "id": "I-z3YLRv1U_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**\n",
        "Feel free to import any additional libraries required"
      ],
      "metadata": {
        "id": "nn_1mKR02Swb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all libraries here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Setting the seed for reproducible results"
      ],
      "metadata": {
        "id": "ugD-twoX2T4N"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Problem 1(a)*\n",
        "\n",
        "In this problem, we shall be exploring the concepts of analytic and numeric computation of gradients for scalar valued functions. \n",
        "\n",
        "\\begin{equation}\n",
        "z = w^{T}x + b \\\\ \n",
        "\\hat{y} = \\sigma(z) = \\frac{1}{1+e^{-z}}\\\\ \n",
        "L(\\hat{y}, y) = y.log(\\hat{y}) + (1-y).log(1-\\hat{y})\n",
        "\\end{equation}\n",
        "\n",
        "For this set of equations, the vector w, and scalars b, y are to be treated as constants. \\\\\n",
        "\n",
        "Now, let us find the analytic gradient of the function L with respect to the function x. That is, compute $\\frac{\\delta L}{\\delta x}$. Write the answer obtained as code in the function provided."
      ],
      "metadata": {
        "id": "LBmB8rFN2XT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise $w$ to a $10 \\times 1$ vector sampled over a standard multivariate gaussian distribution, b to a uniform random variable over the interval $(0, 1)$, y to a bernoulli random variable over $\\{0, 1\\}$"
      ],
      "metadata": {
        "id": "qC-5BMVs-abp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation : \n",
        "w = np.random.normal(size=(10, 1))\n",
        "b = np.random.uniform(0,1)\n",
        "y = np.random.binomial(0,1)"
      ],
      "metadata": {
        "id": "1z1FuiZn-mg1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analytic_grad(x) : \n",
        "\n",
        "    ### WRITE CODE BELOW ###\n",
        "    #calculates the dot product of the transpose of w and x and adds b\n",
        "    z = np.dot(w.T, x) + b\n",
        "    #calculates the sigmoid function using z\n",
        "    sigma = 1 / (1 + np.exp(-z))\n",
        "    #calculates the gradient with respect to sigma\n",
        "    analytic_gradient_wrt_sigma = (sigma - y) / (sigma * (1 - sigma))\n",
        "     #calculates the gradient with respect to z\n",
        "    analytic_gradient_wrt_z = sigma * (1 - sigma)\n",
        "    analytic_gradient_wrt_x = w\n",
        "     #finally calculates the gradient with respect to x\n",
        "    analytic_gradient_wrt_x = np.dot(analytic_gradient_wrt_sigma * analytic_gradient_wrt_z, analytic_gradient_wrt_x)\n",
        "\n",
        "    return analytic_gradient_wrt_x\n",
        "    ### WRITE CODE ABOVE ###"
      ],
      "metadata": {
        "id": "rhgmU3v-6jhd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Problem 1(b)*\n",
        "\n",
        "Now, we compute the numeric gradient for the function L. Refer to [this](https://stackoverflow.com/questions/38854363/is-there-any-standard-way-to-calculate-the-numerical-gradient) stack exchange post to see how numeric gradients are computed"
      ],
      "metadata": {
        "id": "YjAm_AKN7Qk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numeric_grad(x) : \n",
        "\n",
        "    ### WRITE CODE BELOW ###\n",
        "\n",
        "     # first we create x_plus_epsilon and x_minus_epsilon by adding and subtracting epsilon from x \n",
        "    epsilon=1e-5\n",
        "    x_plus_epsilon = x + epsilon\n",
        "    x_minus_epsilon = x - epsilon\n",
        "    \n",
        "    # Calculate the logistic loss function at x_plus_epsilon and x_minus_epsilon\n",
        "    L_plus_epsilon = -y * np.log(1 / (1 + np.exp(-np.dot(w.T, x_plus_epsilon) - b))) - (1 - y) * np.log(1 - 1 / (1 + np.exp(-np.dot(w.T, x_plus_epsilon) - b)))\n",
        "    L_minus_epsilon = -y * np.log(1 / (1 + np.exp(-np.dot(w.T, x_minus_epsilon) - b))) - (1 - y) * np.log(1 - 1 / (1 + np.exp(-np.dot(w.T, x_minus_epsilon) - b)))\n",
        "    \n",
        "    # now finally compute the gradient by taking the finite difference of L_plus_epsilon and L_minus_epsilon\n",
        "    numeric_gradient = (L_plus_epsilon - L_minus_epsilon) / (2 * epsilon)\n",
        "    \n",
        "    return numeric_gradient\n",
        "\n",
        "    ### WRITE CODE ABOVE ###\n",
        "  "
      ],
      "metadata": {
        "id": "Ur4CfvOf7YeR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Problem 2*\n",
        "\n",
        "Here, we'll be looking at computational graphs, and their calculus, finding gradients of  variables with respect to other variables in the graph.\n",
        "\n"
      ],
      "metadata": {
        "id": "MwJI9VYhLjCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be looking at nodes of the graph that are operation based, i.e., each operation performed has a node associated with it.\n",
        "\n",
        "We'll provide you with example implementations for three of the nodes, the *add* node, the *nth power* node and the *sine* node, you'll have to write the classes for all the other nodes."
      ],
      "metadata": {
        "id": "0wwEXqQEREOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-input nodes**"
      ],
      "metadata": {
        "id": "YIeEOTBytyGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Add: \n",
        "  \n",
        "  # A class to add multiple variables together\n",
        "  def __init__(self, lst_names, lst_values):\n",
        "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be added, with each variable name in datatype str.\n",
        "                               # Scalar addition is taken care of in a separate node, we could have fit into this node but thought it might be easier if it wasn't.\n",
        "                               # So cases like \"a + 1\" are to be done separately, and cases like \"b + c + d + 4\" can be done as \"(b + c + d) + 4\" or \"b + c + (d + 4)\", \n",
        "                               #since our scalar addition supports only one variable and one scalar, we'll get to that later\n",
        "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be added, as scalars.\n",
        "\n",
        "  def compute_output(self):\n",
        "    return np.sum(self.lst_values) # This computes the sum of all the variables\n",
        "  \n",
        "  def compute_gradients(self):\n",
        "    return dict(zip(self.lst_names, np.ones(len(self.lst_names)))) # This gives the gradient of the sum wrt to all the input variables, as a dictionary, will be used later\n",
        "\n",
        "\n",
        "class Multiply: \n",
        "  \n",
        "  #Everything's almost the same as the add class, but this deals with multiplication of more than 1 variables\n",
        "  def __init__(self, lst_names, lst_values):\n",
        "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be multiplied, with each variable name in datatype str.\n",
        "                               # Scalar multiplication is taken care of in a separate node\n",
        "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be multiplied, as scalars.\n",
        "\n",
        "  def compute_output(self):\n",
        "    # Write your code to compute the output of this operation\n",
        "    return np.prod(self.lst_values)\n",
        "  def compute_gradients(self):\n",
        "      gradients = {}\n",
        "      for i, name in enumerate(self.lst_names):\n",
        "        temp = np.copy(self.lst_values)\n",
        "        temp[i] = 1\n",
        "        gradients[name] = np.prod(temp)\n",
        "        return gradients\n",
        "    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)"
      ],
      "metadata": {
        "id": "JQuHj5IctK4N"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scalar multiplication/addition nodes**"
      ],
      "metadata": {
        "id": "_QpfDyxht7Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Add_Scalar: \n",
        "  \n",
        "  # A class to add a variable with a scalar\n",
        "  def __init__(self, lst_names, lst_values):\n",
        "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be added, with each variable name in datatype str.\n",
        "                               # This list must only have 2 elements, the first should be the string corresponding to the name of the variable, \n",
        "                               #and the second should be a string of the scalar value to be added.\n",
        "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be added, as scalars.\n",
        "\n",
        "  def compute_output(self):\n",
        "    # Write your code to compute the output of this operation\n",
        "    return self.lst_values[0]+self.lst_values[1]\n",
        "\n",
        "  def compute_gradients(self):\n",
        "    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n",
        "    return {self.lst_names[0]:1}\n",
        "\n",
        "\n",
        "class Multiply_Scalar: \n",
        "  \n",
        "  # A class to multiply a variable with a scalar\n",
        "  def __init__(self, lst_names, lst_values):\n",
        "    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be multiplied, with each variable name in datatype str.\n",
        "                               # This list must only have 2 elements, the first should be the string corresponding to the name of the variable, \n",
        "                               #and the second should be a string of the scalar value to be multiplied.\n",
        "    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be multiplied, as scalars.\n",
        "\n",
        "  def compute_output(self):\n",
        "    # Write your code to compute the output of this operation\n",
        "     return self.lst_values[0]*self.lst_values[1]\n",
        "\n",
        "  def compute_gradients(self):\n",
        "    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n",
        "     return {self.lst_names[0]:self.lst_values[1]}"
      ],
      "metadata": {
        "id": "-0RqnZrStbyI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nodes for special functions**"
      ],
      "metadata": {
        "id": "iF4EA5CLuKXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Power:\n",
        "\n",
        "  def __init__(self, name, value, exponent):\n",
        "    self.name = name # Name of the variable to be exponentiated\n",
        "    self.value = value # Value of the variable\n",
        "    self.exponent = exponent # Value of the exponent\n",
        "  \n",
        "  def compute_output(self):\n",
        "    return np.power(self.value, self.exponent)\n",
        "  \n",
        "  def compute_gradients(self):\n",
        "    return {self.name : self.exponent*(np.power(self.value, (self.exponent - 1)))}\n",
        "\n",
        "class Sine: \n",
        "\n",
        "  # A class to apply the sine function on a variable\n",
        "  def __init__(self, name, value):\n",
        "    self.name = name\n",
        "    self.value = value\n",
        "  \n",
        "  def compute_output(self):\n",
        "    return np.sin(self.value)\n",
        "  \n",
        "  def compute_gradients(self):\n",
        "    return {self.name : np.cos(self.value)}\n",
        "\n",
        "\n",
        "class Logarithm:\n",
        "\n",
        "  # A class to compute the logarithm of a value\n",
        "  def __init__(self, name, value):\n",
        "    self.name = name\n",
        "    self.value = value\n",
        "  \n",
        "  def compute_output(self):\n",
        "    # Write your code here\n",
        "    # Compute_output can be computed using np.log() function\n",
        "    return np.log(self.value)\n",
        "\n",
        "  def compute_gradients(self):\n",
        "    # Write your code here\n",
        "    #compute_gradients computed using derivatives of logarithms function\n",
        "     return {self.name : 1/self.value}\n",
        "\n",
        "class Exponential: \n",
        "\n",
        "  # A class to compute the exponential of a value\n",
        "  def __init__(self, name, value):\n",
        "    self.name = name\n",
        "    self.value = value\n",
        "  \n",
        "  def compute_output(self):\n",
        "    # Write your code here\n",
        "    #compute_output() computed using np.exp function.\n",
        "    return np.exp(self.value)\n",
        "\n",
        "  def compute_gradients(self):\n",
        "    # Write your code here\n",
        "    #compute_gradients() computed using the derivative of the exponential function\n",
        "    return {self.name : np.exp(self.value)}\n"
      ],
      "metadata": {
        "id": "RzhhbukvRBD-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have these classes, let's use them to actually find gradients of complex networks. We finally bring in the idea of a computational graph, which makes it much easier for the gradients to be computed.\n",
        "\n",
        "This is the image of the example graph that we want you to work with. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BsUmSRPKXau8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1VtI1lf85bG8cO1u_8l0D0rqVGhwHQtPK\"\n",
        " width=\"500\" height=\"500\">\n",
        "</div>"
      ],
      "metadata": {
        "id": "e2hDABSUr8Px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "d = 3a \\\\ \n",
        "e = abc \\\\ \n",
        "f = sin(c) \\\\ \n",
        "g = exp(e) \\\\ \n",
        "h = a + d + g + f\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "tPKf5T4lrjxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Propogation**"
      ],
      "metadata": {
        "id": "lnOQqmnlw7eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(a, b, c) : \n",
        "    '''\n",
        "    Input : real numbers a, b, c.\n",
        "\n",
        "    Outputs : A dictionary of the values at each node with keys as the names of nodes\n",
        "    Grads : A dictionary of the gradients at each edge with keys as a pair of nodes \n",
        "    \n",
        "    e.g. Grads[\"ce\"] = ...\n",
        "    '''\n",
        "  \n",
        "    d = 3*a\n",
        "    e = a*b*c\n",
        "    #Calculate value of f as the output of a sine function applied on c\n",
        "    f = Sine(\"c\", c).compute_output()\n",
        "    # Calculate value of g as the output of an exponential function\n",
        "    g = Exponential(\"e\", e).compute_output()\n",
        "    h = a + d + f + g\n",
        "    # Create a dictionary to store the values of all the variables\n",
        "    Outputs = {\"a\": a, \"b\": b, \"c\": c, \"d\": d, \"e\": e, \"f\": f, \"g\": g, \"h\": h}\n",
        "    # Create a dictionary to store the gradients of the final output h \n",
        "    #with respect to all the input variables\n",
        "    Grads = {(\"d\", \"a\"): 3*a,\n",
        "             (\"e\", \"abc\"): (a*b*c),\n",
        "             (\"f\", \"c\"): Sine(\"c\", c).compute_gradients()[\"c\"],\n",
        "             (\"g\", \"e\"): Exponential(\"e\", e).compute_gradients()[\"e\"],\n",
        "             (\"h\", \"d\"): 1,\n",
        "             (\"h\", \"e\"): 1,\n",
        "             (\"h\", \"f\"): 1,\n",
        "             (\"h\", \"a\"): 1,\n",
        "             (\"h\", \"g\"): 1}\n",
        "    return (Outputs, Grads)"
      ],
      "metadata": {
        "id": "GM2n6S4sw-r_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backward Propogation**"
      ],
      "metadata": {
        "id": "ApNrGkHJxjMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the usage of computational graphs in Machine Learning centers around finding the gradients of a particular loss, wrt to all the parameters. Here, your task is to do a simpler version of that.\n",
        "\n",
        "Use the classes you wrote to calculate the following gradients : \n",
        "\n",
        "\n",
        "*   $ \\frac{\\partial h}{\\partial a}$\n",
        "*   $ \\frac{\\partial h}{\\partial b}$\n",
        "*   $ \\frac{\\partial h}{\\partial c}$\n",
        "\n",
        "Use the technique of *backpropogation* to code out the gradients step-wise, along all possible chains of the graph starting from $h$ and ending at $a,b,c$ respectively. \n",
        "\n",
        "Don't try to directly get these values without backpropogation, it might be easier here, but with complicated neural networks it wouldn't be :) \n",
        "\n"
      ],
      "metadata": {
        "id": "450-YOv8fqmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(a, b, c, Outputs, Grads) : \n",
        "    '''\n",
        "    Input : a, b, c (input layer); Outputs (values at each node); Grads (gradients stored at each edge)\n",
        "    Retuns : result (gradients w.r.t a, b, c)\n",
        "    '''\n",
        "    '''To implement the backward propagation, you can use the values stored in the Outputs dictionary along with the gradients stored in the Grads dictionary \n",
        "    to calculate the gradients of the final output (h) with respect to the input variables (a, b, c). \n",
        "    You can use the chain rule of differentiation to compute the gradients along the different paths in the graph.'''\n",
        "    #It uses the gradients stored in the Grads dictionary, from previous code\n",
        "    #which contains the partial derivatives of h with respect to each variable.\n",
        "    #dh_dacalculated by adding the partial derivatives of h with respect to\n",
        "    #a and d, and multiplying the partial derivative of h with respect to d by 3.\n",
        "    dh_da = Grads[(\"h\", \"a\")] + Grads[(\"h\", \"d\")] * 3\n",
        "    #dh_dbb is calculated by multiplying the partial derivative of h \n",
        "    #with respect to e by the product of a and c.\n",
        "    dh_db = Grads[(\"h\", \"e\")] * (a*c)\n",
        "    #dh_dc is calculated by adding the product of the partial derivative of h \n",
        "    #with respect to e and the product of a and b, and \n",
        "    #multiplying the partial derivative of h with respect to f by \n",
        "    #the negative sine of c.\n",
        "    dh_dc = Grads[(\"h\", \"e\")] * (a*b) + Grads[(\"h\", \"f\")] * (-np.sin(c))\n",
        "    #The final result is a list containing the gradients of h with respect to a, b, and c.\n",
        "    result = [dh_da, dh_db, dh_dc]\n",
        "    return result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YwL98tWexpGy"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combining both processes**\n",
        "\n",
        "For the purpose of values, assume that $a = 3, b = 1, c = 2$. Here, we call both forward and backward propogation functions to demonstrate functionality of the functions above."
      ],
      "metadata": {
        "id": "hOsnYr0lx_KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation\n",
        "a = 3\n",
        "b = 1\n",
        "c = 2\n",
        "\n",
        "# Forward propogation\n",
        "(Outputs, Grads) = forward_prop(a, b, c)\n",
        "print(f'Value obtained upon forward propogation : {Outputs[\"h\"]}')\n",
        "\n",
        "# Backward propogation\n",
        "result = backward_prop(a, b, c, Outputs, Grads)\n",
        "print(f'Values of gradients are : {result[0]}, {result[1]}, {result[2]}')"
      ],
      "metadata": {
        "id": "cn-KFyH4yWAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de72da26-1523-4c7f-961a-49cd9c4555c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value obtained upon forward propogation : 416.3380909195608\n",
            "Values of gradients are : 4, 6, 2.090702573174318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submission Instructions**\n",
        "\n",
        "Upload this notebook on your github classroom repository by the name Week1.ipynb"
      ],
      "metadata": {
        "id": "iMTRGZQoy-VU"
      }
    }
  ]
}