{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7DKFv9bQdZ"
      },
      "source": [
        "# **Week 2 : Linear Classifiers and Logistic Regression**\n",
        "\n",
        "In this week we shall explore our first useful machine learning model, namely the linear classifier. The resources for learning about the same can be found [here](https://github.com/Ihsoj-Mahos/WiDS-Week2/tree/master/resources). The objective for this assignment will be to design a linear classifier in order to distinguish a labelled dataset of red and blue points in the $\\mathbb{R}^2$ space. In particular, this assignment deals with the binary classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMiuiv0kaueK"
      },
      "source": [
        "<img src=\"https://stanford.edu/~shervine/teaching/cs-221/illustrations/linear-classifier.png?79f320ac5ba3e9d5dae2c573007dbfb6\"\n",
        " style=\"float:center;width:200px;height:200px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fh8zLgIhM8z"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyqka65hhP-I"
      },
      "outputs": [],
      "source": [
        "# Import Libraries here\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_WYUKncVsl"
      },
      "source": [
        "# **Dataset Generation**\n",
        "\n",
        "First, we will generate a dataset for which the classification problem needs to be solved. In this dataset, each element will be a vector and each label will a either 0 or 1, where 0 corresponds to red and 1 corresponds to blue. You can observe that the data can't be partitioned into two halves exactly. You don't have to edit the cells below.\n",
        "\n",
        "The data is in the form of a $2 \\times n$ matrix, where each column has a pair of $(x,y)$ values. The data is stored in the variable `data`, whereas the labels, a $1 \\times n$ matrix, is stored in the variable `labels` and corresponds to the label {$0,1$} of each point in the data.\n",
        "\n",
        "Note that you don't even have to look at our data generation process, it's fine if you do, but don't waste time on it :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HKUjsCt7a1uV"
      },
      "outputs": [],
      "source": [
        "# Random number generator\n",
        "rng = np.random.default_rng(seed = 1)\n",
        "\n",
        "y_positive = np.abs(rng.normal(0,1,5000)*20)\n",
        "labels = rng.binomial(1,0.95,5000)\n",
        "labels_positive = labels\n",
        "x_positive = rng.normal(0,1,5000)*20\n",
        "\n",
        "y_negative = -1*np.abs(rng.normal(0,1,5000)*20)\n",
        "labels_negative = 1-labels_positive\n",
        "x_negative = rng.normal(0,1,5000)*20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TI8HpPYIxAtR"
      },
      "outputs": [],
      "source": [
        "x = np.concatenate((x_positive, x_negative))\n",
        "y = np.concatenate((y_positive, y_negative))\n",
        "labels = np.concatenate((labels_positive, labels_negative))\n",
        "data_and_labels = np.vstack((x,y,labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ow2TbFNxxB7O"
      },
      "outputs": [],
      "source": [
        "shuffled = data_and_labels[:, np.random.permutation(data_and_labels.shape[1])]\n",
        "x = shuffled[:1,:]\n",
        "y = shuffled[1:2,:]\n",
        "labels = shuffled[2:3,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1LkAPXrxxIgC"
      },
      "outputs": [],
      "source": [
        "theta = np.pi/6\n",
        "\n",
        "rot_matrix = np.array([[np.cos(theta),-1*np.sin(theta)],[np.sin(theta), np.cos(theta)]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yXeeueXKxNbP"
      },
      "outputs": [],
      "source": [
        "data = shuffled[:2,:]\n",
        "data = rot_matrix@data\n",
        "X_plot = data[:1,:]\n",
        "y_plot = data[1:2,:] + 5\n",
        "\n",
        "X = np.vstack((X_plot,y_plot))\n",
        "y = labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "ro1MaqLfxREx",
        "outputId": "0dbb307d-51c6-42f4-c179-a1026be8bc18"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(20,10))\n",
        "\n",
        "colors = ['blue','red']\n",
        "\n",
        "plt.scatter(X_plot, y_plot, c=labels, cmap=matplotlib.colors.ListedColormap(colors), s=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq-0iX-GfN2H"
      },
      "source": [
        "# **Binary Classifier**\n",
        "\n",
        "Now that we have the dataset variable loaded, let's construct the binary classifier using logistic regression. That is we need to estimate the parameters W and b where, \n",
        "\n",
        "\\begin{equation}\n",
        "z = Wx + b \\\\ \n",
        "a = \\sigma(z) = \\frac{1}{1+e^{-z}}\\\\ \n",
        "L(a, y) = -(y.log(a) + (1-y).log(1-a))\n",
        "\\end{equation}\n",
        "\n",
        "In machine-learning terminology, the function $\\sigma(z)$ is called an [activation function](https://en.wikipedia.org/wiki/Activation_function). Activation functions will be covered next week :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf6m3YMPgnZ2"
      },
      "source": [
        "# **Gradient Descent**\n",
        "\n",
        "Here, you have to implement the gradient descent algorithm for the binary classifier and return the parameters W, b. \n",
        "\n",
        "**Bonus (Optional)** : Plot the loss function as a function of the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qLVXGCdQgmRF"
      },
      "outputs": [],
      "source": [
        "def grad_descent(X, y, num_iter = 1000, lr = 0.01) : \n",
        "\n",
        "    # INSERT CODE BELOW\n",
        "    # Initialize the parameters\n",
        "    W = np.zeros((2, 1))\n",
        "\n",
        "    #W = np.zeros((X.shape[0], 1))\n",
        "    b = 0\n",
        "    \n",
        "    # Initialize an empty list to store the loss values for each iteration\n",
        "    loss_values = []\n",
        "    \n",
        "    for i in range(num_iter):\n",
        "        # Compute the dot product of the input features and the parameters\n",
        "        z = np.dot( W.T,X) + b\n",
        "        \n",
        "        # Apply the sigmoid function to get the predicted probability\n",
        "        a = 1 / (1 + np.exp(-z))\n",
        "        \n",
        "        # Compute the loss\n",
        "        loss = - (y * np.log(a) + (1 - y) * np.log(1 - a))\n",
        "        \n",
        "        # Compute the gradient of the loss with respect to the parameters\n",
        "        dW = (1 / X.shape[1]) * np.dot(X, (a - y).T)\n",
        "        db = (1 / X.shape[1]) * np.sum(a - y)\n",
        "        \n",
        "        # Update the parameters\n",
        "        W = W - lr * dW\n",
        "        b = b - lr * db\n",
        "        \n",
        "        # Append the current loss value to the list of loss values\n",
        "        loss_values.append(np.mean(loss))\n",
        "        \n",
        "   \n",
        "    \n",
        "    # INSERT CODE ABOVE\n",
        "\n",
        "    return (W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68v9WhUnhh31"
      },
      "source": [
        "# **Plotting the decision boundary**\n",
        "\n",
        "Given the labelled dataset and the parameters W, b, plot the decision boundary along with the dataset (with the appropriate coloring)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0RFNqRaZOTd-"
      },
      "outputs": [],
      "source": [
        "def plot(X, y, W, b):\n",
        "    # INSERT CODE ABOVE\n",
        "    # Plot the input data\n",
        "    plt.scatter(X[0,:], X[1,:], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral)\n",
        "\n",
        "    # Create a mesh of points over the input space \n",
        "    x_min, x_max = X[0,:].min() - .5, X[0,:].max() + .5\n",
        "    y_min, y_max = X[1,:].min() - .5, X[1,:].max() + .5\n",
        "    h = 0.01\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "     # Flatten the grid of points and stack them with a column of ones\n",
        "    input_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    input_points = np.concatenate((input_points, np.ones((input_points.shape[0], 1))), axis=1)\n",
        "\n",
        "    # Compute the predicted class labels for each point in the mesh\n",
        "    z = np.dot(W.T,input_points) + b\n",
        "    # Apply the sigmoid function to get the predicted probability\n",
        "    a = 1 / (1 + np.exp(-z))\n",
        "    # Convert the predicted probabilities to class labels\n",
        "    # Reshape the class labels back to the grid shape\n",
        "    predicted_labels = np.round(a).reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.contourf(xx, yy, predicted_labels, cmap=plt.cm.Spectral)\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqSCNvUfiDGt"
      },
      "source": [
        "# **Accuracy**\n",
        "\n",
        "Now, let us calculate the accuracy, i.e. percentage of points classified correctly by the classifier given as :\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{accuracy} = 100 * \\frac{\\text{Correctly classified points}}{\\text{Total Number of Points}}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "For this purpose, we will define two functions, for predicting the labels and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GaqLHnmTieXj"
      },
      "outputs": [],
      "source": [
        "def predict(X, y, W, b) : \n",
        "    '''\n",
        "    Inputs \n",
        "    -> X : A numpy array of vectors denoting positions of points\n",
        "    -> y : A numpy array containing labels\n",
        "    -> W, b : Parameters for the model\n",
        "\n",
        "    Returns : \n",
        "    -> A numpy array containing predicted labels for the dataset using the classifier model.\n",
        "    -> Make sure that the dimensions of the input y and the output \"preds\" are the same.\n",
        "    '''\n",
        "\n",
        "    # INSERT CODE BELOW\n",
        "\n",
        "     # Compute the dot product of the input features and the parameters\n",
        "    z = np.dot(W.T, X) + b\n",
        "    \n",
        "    # Apply the sigmoid function to get the predicted probability\n",
        "    a = 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    # Convert the predicted probabilities to class labels\n",
        "    preds = np.round(a)\n",
        "    \n",
        "    # Assert that the dimensions of the input y and the output \"preds\" are the same\n",
        "\n",
        "    # INSERT CODE ABOVE\n",
        "\n",
        "    assert(preds.shape == y.shape)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK4Sxa7fklXH"
      },
      "source": [
        "For the accuracy function, we need to return the accuracy as described by the equation above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "B7ECCVXZkc1Q"
      },
      "outputs": [],
      "source": [
        "def accuracy(X, y, preds) : \n",
        "    '''\n",
        "    Inputs \n",
        "    -> X : A numpy array of vectors denoting positions of points\n",
        "    -> y : A numpy array containing labels\n",
        "    -> preds : Predicted labels by the model\n",
        "\n",
        "    Returns : \n",
        "    -> A floating point number denoting the % accuracy of the model\n",
        "    '''\n",
        "\n",
        "    # INSERT CODE BELOW\n",
        "    #calculate the accuracy of the classifier by dividing the number of \n",
        "    #correctly classified points by the total number of points\n",
        "    ''' it compares the predicted labels with the true labels using the \n",
        "    == operator, this will give you an array of boolean values, where\n",
        "    True means that the prediction is correct, and False means that the\n",
        "    prediction is incorrect. Then it takes the mean of this boolean array,\n",
        "    which will give you the fraction of correct predictions, and \n",
        "    finally, it multiplies this fraction by 100 to get the \n",
        "    accuracy as a percentage.'''\n",
        "    accuracy = (preds == y).mean() * 100\n",
        "\n",
        "\n",
        "    # INSERT CODE ABOVE\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0nl1PKZlDEN"
      },
      "source": [
        "# **Combining the functions**\n",
        "\n",
        "Now, we are done with all the elements we need. Let's combine them into a program. (Feel free to edit the number of iterations to observe how the line changes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "X0qIbHE66cEo",
        "outputId": "15481306-9156-483e-e538-9d1a96a92b3a"
      },
      "outputs": [],
      "source": [
        "# The number of iterations\n",
        "num_iter = 2000\n",
        "\n",
        "W, b = grad_descent(X, y, num_iter)\n",
        "\n",
        "plot(X, y, W, b)\n",
        "predictions = predict(X, y, W, b)\n",
        "result = accuracy(X, y, predictions)\n",
        "\n",
        "print(f'The accuracy of the model is : ', result, ' %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2leM1SSVsFs8"
      },
      "source": [
        "# **Bonus (Optional)**\n",
        "\n",
        "We need to examine how well the model does depending upon the number of iterations, size of input data. So you have to plot the accuracy of the model as the number of iterations varies. This part of the assignment is open-ended and you can choose the sampling of the number of iterations, you can try varying the size of the input data, maybe even a combination of the two! We have not provided a template for this part so you can choose the plotting scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_hZBY1ntCnz"
      },
      "outputs": [],
      "source": [
        "# BONUS PART\n",
        "'''defined an array num_iterations containing a few values of the\n",
        " number of iterations. Then, I use a for loop to iterate over this array,\n",
        " call the grad_descent() function with each value of num_iter, and append \n",
        " the accuracy of the model to the accuracies list.'''\n",
        " \n",
        "num_iterations = [100, 500, 1000, 2000, 5000]\n",
        "accuracies = []\n",
        "\n",
        "for i in num_iterations:\n",
        "    W, b = grad_descent(X, y, num_iter=i, lr=0.01)\n",
        "    preds = predict(X, y, W, b)\n",
        "    acc = accuracy(X, y, preds)\n",
        "    accuracies.append(acc)\n",
        "#create a line plot of the accuracy as a function of the number of iterations.\n",
        "plt.plot(num_iterations, accuracies)\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LBdG5iWLtFiC"
      },
      "source": [
        "# **Submission Instructions**\n",
        "\n",
        "Upload this notebook on your github classroom repository by the name Week2.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "907aabf74e4c7ec249886e417179e944ef5876ac7c1f04eef005a84a557eab63"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
